{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc06934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change file name in line 29, text must be in column 3.\n",
    "# output file is called limit_post.csv. Change this name before you run the script each time.\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import csv\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b575d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/harsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def isNum(passedString):\n",
    "    return any(i.isdigit() for i in passedString)\n",
    "\n",
    "posts_found_with_brand = []\n",
    "limited_post = []\n",
    "limited_post2 = []\n",
    "final_list = []\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = [\"n't\",\"'s\",\"'m\",\"also\",\"'d\",\"'ve\",\"could\",\"would\",\"ca\"]\n",
    "removeStopWords = ['above','below','up','down']\n",
    "stopwords.extend(newStopWords)\n",
    "for word in removeStopWords:\n",
    "    stopwords.remove(word)\n",
    "inter1 = []\n",
    "completeComment_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcce7e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "president = pd.read_pickle('president_df')\n",
    "modi = pd.read_excel('modi.xlsx')\n",
    "\n",
    "inter1 = []\n",
    "for i in modi['Speech']:\n",
    "    inter1.append(i)\n",
    "for i in president['Speech']:\n",
    "    inter1.append(i)\n",
    "    \n",
    "len(inter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d21903b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in inter1:\n",
    "    stop_words = set(stopwords)\n",
    "    sentence_list = sent_tokenize(f)\n",
    "    temp_list = []\n",
    "    for sentence in sentence_list:\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        wordLower_tokens = []\n",
    "        wordLower_tokens = [g.lower() for g in word_tokens]\n",
    "        firstF_list = [h for h in wordLower_tokens if not h in stop_words]\n",
    "        firstF_list = []\n",
    "        secondF_list = []\n",
    "        for i in wordLower_tokens:\n",
    "            if i not in stop_words:\n",
    "                firstF_list.append(i)\n",
    "        for j in firstF_list:\n",
    "            if(isNum(j) == False):\n",
    "                wordNoPunctuation = re.sub('[\\W_]+', '', j)\n",
    "                secondF_list.append(wordNoPunctuation)\n",
    "            else:\n",
    "                secondF_list.append(j)\n",
    "        temp_list.append(secondF_list)\n",
    "    completeComment_list.append(temp_list)\n",
    "posts_found_with_brand = completeComment_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6317ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word list\n",
    "topic1words = ['peace','war','country','security','freedom']\n",
    "topic2words = ['health','tax','economy','education','reform']\n",
    "topic3words = ['order','development','action','trade']\n",
    "topic4words = ['virus','job','money','administration','election']\n",
    "\n",
    "topic_words = [['peace','war','country','security','freedom'],['health','tax','economy','education','reform'],['order','development','action','trade'],['virus','job','money','administration','election']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41dac77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(rating_data):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # these comments are some common operations used to change the vader lexicon\n",
    "\n",
    "    #sid.lexicon.clear()\n",
    "    #newWords = {'positive': 1.0, 'negative': -1.0}\n",
    "    #sid.lexicon.update(newWords)\n",
    "\n",
    "    #score = sid.polarity_scores(sentence)\n",
    "    #compound = score.get('compound')\n",
    "    rating_data['sent_neg'] = -10\n",
    "    rating_data['sent_neu'] = -10\n",
    "    rating_data['sent_pos'] = -10\n",
    "    rating_data['sent_compound'] = -10\n",
    "    for i in range(len(rating_data)):\n",
    "        sentence = rating_data['Sentences'][i]\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        rating_data.iloc[i, 1] = float(ss['neg'])\n",
    "        rating_data.iloc[i, 2] = ss['neu']\n",
    "        rating_data.iloc[i, 3] = ss['pos']\n",
    "        rating_data.iloc[i, 4] = ss['compound']\n",
    "    return rating_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a02255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(topic_words):\n",
    "    limited_post =[]\n",
    "    for post in posts_found_with_brand:\n",
    "        sentence_with_attribute = []\n",
    "        position_in_sentences = []\n",
    "        for sentence in post:\n",
    "            if '' in sentence:\n",
    "                sentence.remove('')\n",
    "                #might have to run seperately for all topics\n",
    "            for target in topic_words:\n",
    "                sentence= ['abcdefgh' if value==target else value for value in sentence]\n",
    "            position = 0\n",
    "            for i in sentence:\n",
    "                if(i=='abcdefgh'):\n",
    "                    position_in_sentences.append(position)\n",
    "                    sentence_with_attribute.append(sentence)\n",
    "                position = position + 1\n",
    "        j = 0\n",
    "\n",
    "        for sentence in sentence_with_attribute:\n",
    "            limited_sentence = []\n",
    "            for i in range(len(sentence)):\n",
    "                if(abs(i - position_in_sentences[j]) < 3):\n",
    "                    limited_sentence.append(sentence[i])\n",
    "            j=j+1\n",
    "            limited_post.append(limited_sentence)\n",
    "    for sent in limited_post:\n",
    "        limited_post2.append(' '.join(sent))\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['Sentences']  = limited_post2\n",
    "    \n",
    "    rating_data=df\n",
    "    # rating_data = pd.read_csv(\"limit_post.csv\")\n",
    "    rating_data = rating_data.rename(columns={ rating_data.columns[0]: \"Sentences\" })\n",
    "\n",
    "    sentiment_data = get_sentiment(rating_data)\n",
    "    return(sentiment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6ad4af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['topic1_sentiment.csv','topic2_sentiment.csv','topic3_sentiment.csv','topic4_sentiment.csv']\n",
    "for i in range(0, len(topic_words)):\n",
    "    result = find_sentiment(topic_words[i])\n",
    "    result.to_csv(names[i])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a14fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
